---
title: "Themenmodellierung"
subtitle: "Automatische Identifikation und Strukturierung von Themen in Texten"
figtitle: "üì∞"
draft: false
toc_show: true
---

## Was ist Themenmodellierung?

<!-- {{{< div class="grid" >}}} {{{< div class="g-col-12 g-col-md-4 g-start-md-5" >}}}  -->

<!-- ```{r} -->
<!-- knitr::include_graphics(here::here("content/methods/topic/img/book.svg")) -->
<!-- ``` -->

<!-- {{{< /div >}}} {{{< /div >}}} -->

Themenmodellierung ist eine Methode des maschinellen Lernens, mit der aus gro√üen Textsammlungen automatisch thematische Strukturen extrahiert werden k√∂nnen. Dabei werden Texte so analysiert, dass latente Themen sichtbar gemacht werden, ohne dass eine vorherige Kategorisierung notwendig ist. Diese Methode ist besonders n√ºtzlich, um gro√üe Mengen unstrukturierter Daten besser zu verstehen und effizient zu organisieren.



## Anwendungen 

{{{< fa-ul >}}}
    {{{< fa-solid-li icon="star" >}}}**Kundenfeedback analysieren:** Unternehmen k√∂nnen Kundenrezensionen, Support-Tickets oder Social-Media-Kommentare automatisch auswerten und h√§ufige Themen identifizieren. So lassen sich Verbesserungspotenziale oder neue Kundenbed√ºrfnisse schneller erkennen.{{{< /fa-solid-li >}}}
    {{{< fa-solid-li icon="square-poll-vertical" >}}}**Marktforschung und Wettbewerbsanalyse:** Durch die Analyse von Nachrichtenartikeln, Blogs und Forenbeitr√§gen k√∂nnen Unternehmen Branchentrends fr√ºhzeitig erkennen und ihre Strategien entsprechend anpassen.{{{< /fa-solid-li >}}}
    {{{< fa-solid-li  icon="envelope" >}}}**Automatische Kategorisierung von Support-Anfragen:** Unternehmen mit vielen Kundenanfragen k√∂nnen durch Themenmodellierung h√§ufige Probleme identifizieren und diese automatisiert an die zust√§ndigen Abteilungen weiterleiten. Dies verbessert die Reaktionszeit und steigert die Effizienz im Kundenservice.{{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}


## Ans√§tze zur Themenmodellierung

Es gibt verschiedene Ans√§tze zur Themenmodellierung, die je nach Anwendungsfall unterschiedliche Vor- und Nachteile bieten. Klassische Modelle basieren h√§ufig auf Wahrscheinlichkeitsverteilungen, w√§hrend moderne Methoden neuronale Netze nutzen, um semantische Zusammenh√§nge besser zu erfassen.

### Klassische probabilistische Modelle

Probabilistische Modelle verwenden statistische Verfahren, um Dokumente als Mischung verschiedener Themen zu modellieren. Diese Modelle haben sich insbesondere in der automatischen Textklassifikation bew√§hrt.

{{{< p class="pb-0" >}}}
Latent Dirichlet Allocation (LDA) ist eines der bekanntesten probabilistischen Modelle f√ºr die Themenmodellierung. Es geht davon aus, dass jedes Dokument eine Mischung aus verschiedenen Themen ist und jedes Wort mit einer bestimmten Wahrscheinlichkeit zu einem dieser Themen geh√∂rt. Dies bedeutet, dass Dokumente nicht auf ein einziges Thema beschr√§nkt ist, sondern vielmehr aus unterschiedlichen thematischen Anteilen bestehen k√∂nnen. So k√∂nnte etwa ein Dokument zu 10% dem Thema "Digitalisierung" und zu 90% dem Thema "Kundenservice" zugeordnet sein, w√§hrend ein anderes Dokument dieselben Themen in einem Verh√§ltnis von 65% zu 35% behandelt. LDA wird h√§ufig zur automatischen Dokumentenkategorisierung und -zusammenfassung eingesetzt.
{{{< /p >}}}

{{{< div class="grid my-4" >}}} {{{< div class="g-col-12 g-col-md-10 g-start-md-2" >}}}

```{r}
knitr::include_graphics(here::here("content/methods/topic/img/lda_topic.svg"))
```

{{{< /div >}}} {{{< /div >}}}

#### Vor- und Nachteile

{{{< fa-ul >}}}
    {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Dokumente k√∂nnen simultan mehrere Themen abbilden, was die Komplexit√§t realer Texte oftmals pr√§ziser widerspiegelt. {{{< /fa-solid-li >}}}
        {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Als Verfahren des un√ºberwachten maschinellen Lernens erfordert LDA keine vordefinierten Themenkategorien oder manuelle Klassifizierung von Trainingsdaten. {{{< /fa-solid-li >}}}
    {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Die optimale Themenanzahl muss a priori festgelegt werden, obwohl hierf√ºr keine eindeutigen Bestimmungskriterien existieren. {{{< /fa-solid-li >}}}
        {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Die Themen m√ºssen selbst interpretiert werden, da der Algorithmus lediglich repr√§sentative Begriffskombinationen ohne inhaltliche Deutung liefert.  {{{< /fa-solid-li >}}}
         {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Bei sehr kurzen Texten, wie etwa Social-Media-Beitr√§gen, zeigt die Methode erhebliche Schw√§chen in der Themenextraktion. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}



### Kontextuelle Themenmodellierung

Moderne Themenmodellierungsverfahren setzen zunehmend auf vortrainierte Sprachmodelle, wie {{{< crossref path="/basics#bert" label="BERT" >}}}, um semantische Informationen noch pr√§ziser zu extrahieren.

BERTopic ist ein modernes, auf neuronalen Netzen basierendes Verfahren, das die St√§rken von Transformer-Modellen mit Clustering-Techniken kombiniert. Dadurch lassen sich kontextuelle Wortbedeutungen erkennen und thematische Strukturen auch in kurzen, komplexen Texten erfassen. Im Gegensatz zu klassischen Methoden wie LDA kann BERTopic flexibler auf neue Themen reagieren und bietet eine feinere semantische Differenzierung.

Die Themenmodellierung erfolgt dabei in vier aufeinander aufbauenden Schritten:

{{{< fa-ul >}}}
    {{{< fa-solid-li icon="robot" >}}}**Embeddings:** Jeder Text wird vom Sprachmodell (BERT) in einen Zahlenvektor (Embedding) √ºbersetzt, der seine Bedeutung im Kontext abbildet. {{{< /fa-solid-li >}}}
    {{{< fa-solid-li icon="down-long" >}}}**Dimensionsreduktion:** Diese aus dem ersten Schritt resultierenden hochdimensionalen Vektoren werden in wenige Dimensionen reduziert.{{{< /fa-solid-li >}}}
    {{{< fa-solid-li  icon="circle-nodes" >}}}**Cluster:** Semantisch √§hnliche Texte werden automatisch zu thematischen Gruppen zusammengefasst.{{{< /fa-solid-li >}}}
    {{{< fa-solid-li  icon="list" >}}}**Themenbeschreibung:** Repr√§sentative Begriffe pro Gruppe helfen dabei, ein passendes Thema zu identifizieren. Diese Begriffe werden f√ºr jedes Cluster mithilfe der {{{< crossref path="/basics#tf-idf" label="TF-IDF Methode" >}}} ermittelt. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}


{{{< div class="grid my-4" >}}} {{{< div class="g-col-12 g-col-md-10 g-start-md-2" >}}}

```{r}
knitr::include_graphics(here::here("content/methods/topic/img/bertopic.svg"))
```

{{{< /div >}}} {{{< /div >}}}



#### Vor- und Nachteile

{{{< fa-ul >}}}
    {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} BERTopic ben√∂tigt keine aufwendige Vorverarbeitung von Texten, da der Kontext ber√ºcksichtigt wird. Es sogar nachteilig sein, W√∂rter zu entfernen oder stark zu vereinfachen. {{{< /fa-solid-li >}}}
    {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Funktionieren zuverl√§ssig auch bei kurzen oder informell geschriebenen Texten, wo LDA oft scheitert. {{{< /fa-solid-li >}}}
        {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} K√∂nnen mittels Fine Tuning und spezifischer Daten auf eigene, relevante Themen angepasst werden. {{{< /fa-solid-li >}}}
    {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Da BERTopic aus mehreren aufeinander aufbauenden Verfahren besteht, erfordert die Umsetzung technisches Know-how, insbesondere zur Auswahl der Parameter. {{{< /fa-solid-li >}}}
        {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Die Rechenleistung ist h√∂her, insbesondere bei gro√üen Datens√§tzen.  {{{< /fa-solid-li >}}}
         {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}}	Wie auch bei LDA m√ºssen die Themen selbst interpretiert werden. Die Zuordnung bleibt also subjektiv. {{{< /fa-solid-li >}}}
         {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}}	BERTopic kann zu vielen sehr kleinen Themen mit wenigen Texten f√ºhren, was die Interpretation erschwert. Eine Anpassung der Modellparameter hilft, diese zu b√ºndeln. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}


## Exemplarische Anwendung mit BERTopic

Auf dieser Seite k√∂nnen Sie ein BERTopic-Modell in Aktion erleben. Das Modell wurde auf einer Sammlung von Zeitungsartikeln trainiert, und die Ergebnisse werden in einer interaktiven Visualisierung dargestellt.

### Ausprobieren

Geben Sie einen beliebigen Text in das untenstehende Feld ein, und unser BERTopic-Modell wird ihn analysieren und das wahrscheinlichste Thema bestimmen.

```{r}
nd.util:::nd_iframe_app(
  .url = "https://shiny.dsjlu.wirtschaft.uni-giessen.de/ner_demo/",
  .height = "4rem"
)
```

