---
title: "Entit√§tsanalyse"
subtitle: "Automatisierte Identifikation und Klassifikation von Entit√§ten und Eigennamen"
figtitle: "üè∑Ô∏è"
date: "2024-11-25T14:25:49+01:00"
toc_show: true
draft: true
html-table-processing: none
---

```{r}
library(tidyverse)
```

## Was ist Entit√§tsanalyse?

Die Entit√§tsanalyse ist eine NLP-Methode zur automatischen Erkennung und Klassifizierung wichtiger Begriffe (z.B. Personen, Orte, Organisationen) in unstrukturierten Texten. Als Entit√§t bezeichnet man dabei die Information, die aus dem Text extrahiert werden soll. Eine Entit√§t stellt dabei einen Begriff dar, der in einem Text konsequent das gleiche bezeichnet. So lassen sich mittels Entit√§tsanalyse beispielsweise Namen von Personen, Firmennamen, Orte, Ereignisse oder Zeitangaben erkennen und extrahieren. Daf√ºr sind zwei Schritte grundlegend: Die Identifikation von Entit√§ten im Text und deren Zuordnung zu vordefinierten Kategorien.

{{{< div class="grid my-4" >}}} {{{< div class="g-col-12" >}}} 

```{r}
knitr::include_graphics(here::here("content/methods/ner/img/ner.svg"))
```

{{{< /div >}}} {{{< /div >}}}

## Anwendungen

{{{< fa-ul >}}}
  {{{< fa-solid-li icon="star"  >}}} **Identifikation sensibler Informationen:** Mittels Entit√§tsanalyse lassen sich personenbezogene Daten wie Namen oder Adressen automatisch erkennen und pseudonymisieren.
  {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="lightbulb"  >}}} **Wettbewerbsanalyse:** Durch das Extrahieren von Mitbewerber- oder Produktnamen lassen sich relevante Informationen f√ºr Marktanalysen gewinnen.
  {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="ticket-simple" >}}} **Dokumentenklassifikation:** Die Entit√§tsanalyse erkennt relevante Begriffe wie ‚ÄûRechnung‚Äú oder ‚ÄûProjektbericht‚Äú und unterst√ºtzt so die automatische Kategorisierung von Dokumenten. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}


## Ans√§tze zur Entit√§tsanalyse

<!-- TODO: Reichen die beiden Ans√§tze? Oder auch "machine learning" basiert (zB Spacy)? -->

F√ºr diese Methode gibt es verschiedene Algorithmen, aus denen Unternehmen je nach Bedarf und verf√ºgbaren Ressourcen w√§hlen k√∂nnen: 

### Regelbasierte Entit√§tsanalyse 

Regelbasierte Ans√§tze der Entit√§tsanalyse nutzen vordefinierte Muster und W√∂rterlisten, um Entit√§ten wie Personen, Organisationen oder Orte im Text zu identifizieren. Dabei werden meist zwei Verfahren kombiniert: Lexikonbasierte Erkennung und musterbasierte Regeln.

```{r}
entity_example <- tibble(
  Entit√§t = c(
    "Organisation",
    "Person",
    "Ortsname",
    "Land",
    "Datum",
    "Uhrzeit",
    "Produkt",
    "Menge",
    "W√§hrung",
    "E-Mail"
  ),
  Beispiel = c(
    "JLU",
    "Max Mustermann",
    "Gie√üen",
    "Deutschland",
    "24.05.2024",
    "13:30",
    "Laptop",
    "20.000",
    "Euro",
    "mustermann@beispiel.de"
  )
)

entity_example |>
  gt::gt() |>
  gt::tab_options(
    table.width = gt::pct(50),
    data_row.padding = gt::px(4)
  ) |>
  nd.util::nd_gt_tab_options() |>
  nd.util::nd_gt_to_html()

```

Beim lexikonbasierten Ansatz wird der Text mit einem vordefinierten Lexikon abgeglichen, also mit Listen relevanter Namen und Begriffe. Stimmen W√∂rter im Text mit diesen Eintr√§gen √ºberein, werden sie als Entit√§t klassifiziert. Erg√§nzend kommen regelbasierte Methoden zum Einsatz, die sprachliche Strukturen analysieren. Diese Regeln basieren oft auf linguistischen Mustern, wie z.B. Gro√üschreibung innerhalb eines Satzes oder typischen Formatierungen wie Datums- oder E-Mail-Strukturen.

{{{< fa-ul >}}}
  {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} L√§sst sich unkompliziert umsetzen, selbst mit begrenzten Ressourcen oder ohne eigene Trainingsdaten. {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Kann gezielt auf spezifische Anwendungsf√§lle zugeschnitten werden, etwa durch eigene Lexika oder regelbasierte Anpassungen. {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Erfordert kontinuierliche Pflege, da Regeln und Lexika regelm√§√üig aktualisiert werden m√ºssen. {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Die Qualit√§t h√§ngt stark von der Sprache und den verwendeten W√∂rterb√ºchern ab. {{{< /fa-solid-li >}}}
  {{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Sprachliche Varianten und Ausnahmen lassen sich nicht immer zuverl√§ssig erfassen. Dadurch k√∂nnen Entit√§ten √ºbersehen werden. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}

### Kontextuelle Entit√§tsanalyse

Die Entit√§tsanalyse ist ein weiterer zentraler Anwendungsfall f√ºr vortrainierte Sprachmodelle, wie bereits bei der Sentimentanalyse und Themenmodellierung gezeigt. Es stehen Modelle zur Verf√ºgung, die speziell f√ºr die Erkennung benannter Entit√§ten trainiert wurden. Alternativ k√∂nnen Unternehmen bestehende vortrainierte Modelle √ºbernehmen und mittels Fine-Tuning gezielt an ihre individuellen Anforderungen anpassen.

Besonders empfehlenswert sind Modelle aus der BERT-Familie, da sie speziell f√ºr Klassifikationsaufgaben wie die Entit√§tsanalyse konzipiert wurden und kontextuelle Informationen zuverl√§ssig verarbeiten k√∂nnen.

<!-- TODO: add cross refs  -->

<!-- TODO: Add illustration for kontextuelle Entit.  -->

{{{< fa-ul >}}}
{{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Analysiert die Bedeutung eines Wortes im Zusammenhang des Satzes und erkennt dadurch unterschiedliche Verwendungen. {{{< /fa-solid-li >}}}
{{{< fa-solid-li icon="thumbs-up" color="#39b185" >}}} Vortrainierte Modelle k√∂nnen durch Fine-Tuning an spezifische Dom√§nen angepasst werden. {{{< /fa-solid-li >}}}
{{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Erfodert mehr Rechenressourcen im Vergleich zu regelbasierten Verfahren. {{{< /fa-solid-li >}}}
{{{< fa-solid-li icon="thumbs-down" color="#cf597e" >}}} Wenn Modelle in der Cloud laufen (z. B. √ºber APIs), kann das problematisch sein, wenn sensible Gesch√§ftsdaten oder Kundendaten verarbeitet werden. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}


Um den Unterschied der beiden Methoden zu verdeutlichen, zeigen wir, wie jede Methode den folgendenen S√§tze klassifizieren w√ºrde: 

{{{< fa-ul >}}}
  {{{< fa-solid-li >}}} Ich habe mein Buch auf der **Bank** liegen lassen. {{{< /fa-solid-li >}}}
  {{{< fa-solid-li >}}} Die **Bank** schlie√üt bald. {{{< /fa-solid-li >}}}
{{{< /fa-ul >}}}

Ein regelbasierter Ansatz w√ºrde beide Vorkommen von Bank als Finanzinstitut klassifizieren, wenn das Wort im Lexikon so hinterlegt ist. Ein kontextueller Ansatz erkennt dagegen den Unterschied, da es dein Kontext um den Begriff ber√ºcksichtigt.



## Exemplarische Anwendung mit BERT

F√ºr die folgende Demo kommt ein Sprachmodell zum Einsatz, das speziell f√ºr die **Entit√§tsanalyse** trainiert wurde. Es kann in deutschsprachigen Texten automatisch Personen, Orte, Organisationen oder andere wichtige Begriffe identifizieren.  


### Ausprobieren

Klicken Sie auf **Zuf√§lliger Google News Artikel**, um einen Artikel mit farblich hervorgehobenen Entit√§ten anzuzeigen.

```{r}
nd.util:::nd_iframe_app(
  .url="https://shiny.dsjlu.wirtschaft.uni-giessen.de/ner_demo/",
  .height="4rem"
)
```
